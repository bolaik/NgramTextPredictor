probBigramPredictor <- function(input, predict) {
#
# single word input plus a predict word, output the probability
# katz backoff with good-turing discounting
#
bigram.in <- subset(bigram.wf, unigram == input)
if(nrow(bigram.in) > 0) {
bigram.record <- subset(bigram.in, lastword == predict)
if(nrow(bigram.record) > 0) {
# find bigram record
freq.tot <- sum(bigram.in$freq)
freq.eff <- bigram.record$freq * bigram.record$discount
prob.final <- freq.eff / freq.tot
} else {
# not find bigram record, go to unigram
beta <- subset(bigram.beta, unigram == input)$probLeftover
unigram.in <- unigram.wf
unigram.record <- subset(unigram.wf, words == predict)
if(nrow(unigram.record) > 0) {
# found in unigram
unigram.in.remain <- unigram.in[!(unigram.in$words %in% bigram.in$lastwords),]
freq.tot <- sum(unigram.in$freq)
freq.eff <- sum(unigram.in.remain$freq * unigram.in.remain$discount)
alpha <- beta / (freq.eff / freq.tot)
freq.eff.record <- unigram.record$freq * unigram.record$discount
prob.final <- alpha * (freq.eff.record / freq.tot)
} else {
stop(sprintf("predictor [%s] not found in bigram model", predict))
}
}
} else {
sprintf("input [%s] not found in bigram model.", input)
probUnigramPredictor(predict)
}
prob.final
}
probUnigramPredictor <- function(predict) {
#
# output probability of given predict word with unigram data
#
unigram.in <- unigram.wf
unigram.record <- subset(unigram.wf, words == predict)
if(nrow(unigram.record) > 0) {
freq.tot <- sum(unigram.in)
freq.eff <- unigram.record$freq * unigram.record$discount
prob.final <- freq.eff / freq.tot
} else {
# any predicted word should be found in the training corpus
# otherwise no way to predict
stop(sprintf("predictor [%s] not found in unigram model", predict))
}
prob.final
}
```{r predict, message=FALSE, eval=FALSE}
# prepare bigram and trigram word frame for future manipulation
#
library(dplyr)
trigram.wf <- read.csv("../trial-ngram/trigram-wf.csv", stringsAsFactors = FALSE)
trigram.fft <- read.csv("../trial-ngram/trigram-fft.csv")
trigram.wf$bigram <- gsub(" [^ ]+$", "", trigram.wf$words)
trigram.wf$unigram <- gsub(".* ", "", trigram.wf$bigram)
trigram.wf$lastword <- gsub(".* ", "", trigram.wf$words)
trigram.wf <- merge(trigram.wf, trigram.fft[,c(1,3)], by = "freq")
# trigram leftover probability
trigram.beta <- trigram.wf %>% group_by(bigram) %>%
summarise(probLeftover = probLeftover(freq, discount)) %>%
as.data.frame()
bigram.wf <- read.csv("../trial-ngram/bigram-wf.csv", stringsAsFactors = FALSE)
bigram.fft <- read.csv("../trial-ngram/bigram-fft.csv")
bigram.wf$unigram <- gsub(" .*", "", bigram.wf$words)
bigram.wf$lastword <- gsub(".* ", "", bigram.wf$words)
bigram.wf <- merge(bigram.wf, bigram.fft[,c(1,3)], by = "freq")
# bigram leftover probability
bigram.beta <- bigram.wf %>% group_by(unigram) %>%
summarise(probLeftover = probLeftover(freq, discount)) %>%
as.data.frame()
unigram.wf <- read.csv("../trial-ngram/unigram-wf.csv", stringsAsFactors = FALSE)
unigram.fft <- read.csv("../trial-ngram/unigram-fft.csv")
unigram.wf <- merge(unigram.wf, unigram.fft[,c(1,3)], by = "freq")
# simple version of prediction function
predict.unigram <- arrange(unigram.wf, desc(freq * discount)) %>%
select(words) %>% head(10)
predict.unigram <- predict.unigram$words
nextWordPrediction <- function(rawinput, numRecommend = 6) {
input.list <- cleanInput(rawinput)
input <- input.list[[1]]
isTrigram <- input.list[[2]]
if(isTrigram == TRUE) {
# pre-selection of 20 words as candidate predict word
# bare Good-Turing discounted probabilities are used
trigram.in <- subset(trigram.wf, bigram == input)
predict.trigram <- arrange(trigram.in, desc(freq * discount)) %>%
select(lastword) %>% head(10)
predict.trigram <- predict.trigram$lastword
input.uni <- gsub(".* ", "", input)
bigram.in <- subset(bigram.wf, unigram == input.uni)
predict.bigram <- arrange(bigram.in, desc(freq * discount)) %>%
select(lastword) %>% head(10)
predict.bigram <- predict.bigram$lastword
predict.vector <- unique(c(predict.trigram, predict.bigram, predict.unigram))
# katz backoff for advanced search
prob.vec <- sapply(predict.vector, function(x) probTrigramPredictor(input, x))
prob.df <- data.frame(predictor = predict.vector, probability = prob.vec)
wordRecommend <- arrange(prob.df, desc(prob.vec)) %>% head(numRecommend) %>% t()
} else {
input.uni <- gsub(".* ", "", input)
bigram.in <- subset(bigram.wf, unigram == input.uni)
predict.bigram <- bigram.in$lastword
predict.vector <- unique(c(predict.bigram, predict.unigram))
# katz backoff
prob.vec <- sapply(predict.vector, function(x) probBigramPredictor(input.uni, x))
prob.df <- data.frame(predictor = predict.vector, probability = prob.vec)
wordRecommend <- arrange(prob.df, desc(prob.vec)) %>% head(numRecommend) %>% t()
}
# vector of suggested words with probability
wordRecommend
}
nextWordPrediction("this is")
nextWordPrediction("i love")
nextWordPrediction("shame of")
nextWordPrediction("hard to")
nextWordPrediction("try it")
?xtable
nextWordPrediction("you're so")
nextWordPrediction("fuck")
nextWordPrediction("merry")
nextWordPrediction("water")
nextWordPrediction("iphone")
nextWordPrediction("new phone")
q()
library(tm)
library(filehash)
fname <- "../../final/en_US/"
# load English txt from blogs, news and twitters
docs <- PCorpus(DirSource(fname, encoding="UTF-8", mode="text"),
readerControl=list(language="en"),
dbControl = list(dbName = "docs_origin.db", dbType = "DB1"))
# define custom view of documents in the corpus
viewDocs <- function(doc, entry, lines)
{txt <- as.character(doc[[entry]])[lines]; writeLines(strwrap(txt))}
viewDocs(docs, 1, 10)
viewDocs(docs, 2, 10)
viewDocs(docs, 3, 10)
viewDocs(docs, 3, 1)
viewDocs(docs, 2, 1)
viewDocs(docs, 1, 1)
viewDocs(docs, 2, 5)
viewDocs(docs, 3, 5)
viewDocs(docs, 1, 5)
viewblg <- viewDocs(docs, 1, 5)
viewnew <- viewDocs(docs, 2, 5)
viewtwt <- viewDocs(docs, 3, 5)
library(xtable)
print(xtable(rbind(viewblg, viewnew, viewtwt)), comment=FALSE)
viewDocs <- function(doc, entry, lines)
{txt <- as.character(doc[[entry]])[lines]; strwrap(txt)}
viewblg <- viewDocs(docs, 1, 5)
viewnew <- viewDocs(docs, 2, 5)
viewtwt <- viewDocs(docs, 3, 5)
library(xtable)
print(xtable(rbind(viewblg, viewnew, viewtwt)), comment=FALSE)
?rbind
?xtable
?list
print(xtable(list(viewblg, viewnew, viewtwt)), comment=FALSE)
viewDocs <- function(doc, entry, lines)
{txt <- as.character(doc[[entry]])[lines]; txt}
viewblg <- viewDocs(docs, 1, 5)
viewnew <- viewDocs(docs, 2, 5)
viewtwt <- viewDocs(docs, 3, 5)
library(xtable)
print(xtable(rbind(viewblg, viewnew, viewtwt)), comment=FALSE)
?row.names
?histogram
?hist
?scatter.smooth
?plot
dirtrialngram <- "../trial-ngram/"
unigram.fft <- read.csv(file = paste(dirtrialngram, "unigram-fft.csv", sep = ""))
bigram.fft  <- read.csv(file = paste(dirtrialngram, "bigram-fft.csv" , sep = ""))
trigram.fft <- read.csv(file = paste(dirtrialngram, "trigram-fft.csv", sep = ""))
# scatter plot
library(scales)
par(mfrow = c(1,3))
scatter.smooth(log10(unigram.fft$freq), log10(unigram.fft$freq.freq), col = alpha("blue", 0.1),
xlab = "Frequency", ylab = "Frequency of frequency", main = "Unigram")
scatter.smooth(log10(bigram.fft$freq), log10(bigram.fft$freq.freq), col = alpha("yellow", 0.1),
xlab = "Frequency", ylab = "Frequency of frequency", main = "Bigram")
scatter.smooth(log10(trigram.fft$freq), log10(trigram.fft$freq.freq), col = alpha("magenta", 0.1),
xlab = "Frequency", ylab = "Frequency of frequency", main = "Trigram")
par(mfrow = c(1,3))
scatter.smooth(log10(unigram.fft$freq), log10(unigram.fft$freq.freq), col = alpha("blue", 0.1),
xlab = "Frequency", ylab = "Frequency of frequency", main = "Unigram",
xlim = c(0,4), ylim = c(0,6))
scatter.smooth(log10(bigram.fft$freq), log10(bigram.fft$freq.freq), col = alpha("yellow", 0.1),
xlab = "Frequency", ylab = "Frequency of frequency", main = "Bigram",
xlim = c(0,4), ylim = c(0,6))
scatter.smooth(log10(trigram.fft$freq), log10(trigram.fft$freq.freq), col = alpha("magenta", 0.1),
xlab = "Frequency", ylab = "Frequency of frequency", main = "Trigram",
xlim = c(0,4), ylim = c(0,6))
dirtrialngram <- "../trial-ngram/"
unigram.fft <- read.csv(file = paste(dirtrialngram, "unigram-fft.csv", sep = ""))
bigram.fft  <- read.csv(file = paste(dirtrialngram, "bigram-fft.csv" , sep = ""))
trigram.fft <- read.csv(file = paste(dirtrialngram, "trigram-fft.csv", sep = ""))
# scatter plot
library(scales)
par(mfrow = c(1,3))
scatter.smooth(log10(unigram.fft$freq), log10(unigram.fft$freq.freq), col = alpha("blue", 0.1),
xlab = "Frequency", ylab = "Frequency of frequency", main = "Unigram",
xlim = c(0,4), ylim = c(0,6))
scatter.smooth(log10(bigram.fft$freq), log10(bigram.fft$freq.freq), col = alpha("blue", 0.1),
xlab = "Frequency", ylab = "Frequency of frequency", main = "Bigram",
xlim = c(0,4), ylim = c(0,6))
scatter.smooth(log10(trigram.fft$freq), log10(trigram.fft$freq.freq), col = alpha("blue", 0.1),
xlab = "Frequency", ylab = "Frequency of frequency", main = "Trigram",
xlim = c(0,4), ylim = c(0,6))
library(readr)
unigram_wf <- read_csv("~/R/JHU/Data Science Capstone/nlp-wei/trial-ngram/unigram-wf.csv")
View(unigram_wf)
q()
?xtable
library(xtable)
?xtable
library(tm)
library(filehash)
cname <- "../clean/"
docs <- PCorpus(DirSource(cname, encoding="UTF-8", mode="text"),
readerControl=list(language="en"),
dbControl = list(dbName = "docs_clean.db", dbType = "DB1"))
library(tau)
library(xtable)
ngram <- function(corpus, n) {
textcnt(corpus, method = "string", n = as.integer(n), split = "[ ]+", decreasing = TRUE)
}
# analysis of words in corpus
word.blog  <- ngram(docs[[1]][[1]], 1)
word.blog  <- data.frame(words = names(word.blog) ,  counts = unclass(word.blog))
word.news  <- ngram(docs[[2]][[1]], 1)
word.news  <- data.frame(words = names(word.news) ,  counts = unclass(word.news))
word.tweet <- ngram(docs[[3]][[1]], 1)
word.tweet <- data.frame(words = names(word.tweet), counts = unclass(word.tweet))
unique.word <- c(length(word.blog$words), length(word.news$words), length(word.tweet$words))
total.word <- c(sum(word.blog$counts), sum(word.news$counts), sum(word.tweet$counts))
summaryCorpus <- data.frame(Word = total.word, Vocabulary = unique.word,
typeTokenRatio = unique.word / total.word)
row.names(summaryCorpus) <- c("Blogs", "News", "Tweets")
summaryCorpus
xtable(summaryCorpus, digits = 4)
length(word.blog$words)
dim(word.blog)[1]
library(readr)
bigram_wf <- read_csv("~/R/DataScienceJHU/Data Science Capstone/nlp-wei/trial-ngram/bigram-wf.csv")
View(bigram_wf)
install.packages("tufte")
install.packages("rticles")
install.packages("rmdformats")
q()
q()
probLeftover <- function(frequency, discount) {
#
# calculate left-over probability
#
freq.tot <- sum(frequency)
freq.eff <- sum(frequency * discount)
return(1-freq.eff/freq.tot)
}
cleanInput <- function(input, maxN = 2) {
#
# preprocessing user input string
#
input <- tolower(input)
input <- gsub("[^[:alnum:][:space:]',.?!:-]", "", input)
input <- gsub("\\.", " \\. ", input)
input <- gsub("\\,", " \\, ", input)
input <- gsub("\\!", " \\! ", input)
input <- gsub("\\?", " \\? ", input)
input <- gsub("\\:", " \\: ", input)
input <- gsub("^[[:space:]]+", "", input)
input <- gsub("[[:space:]]+", " ", input)
# tokenize
words <- unlist(strsplit(input, split = " "))
# output bigram if sufficient words
# otherwise unigram
if(length(words) < maxN) {
isTrigram = FALSE
return(list(input, isTrigram))
}else{
isTrigram = TRUE
bgn <- length(words)-maxN+1
end <- length(words)
tempwords <- words[bgn:end]
return(list(paste(tempwords, collapse = " "), isTrigram))
}
}
probTrigramPredictor <- function(input, predict) {
#
# with bigram input plus a predict word, output the probability
# estimator from Katz back-off method (together with Good-Turing discounting)
#
# e.g. Input: want to
#      predict: sleep
#      output: 0.04536
#
trigram.in <- subset(trigram.wf, bigram == input)
if(nrow(trigram.in) > 0) {
trigram.record <- subset(trigram.in, lastword == predict)
if(nrow(trigram.record) > 0) {
# trigram found in model
freq.tot <- sum(trigram.in$freq)
freq.eff <- trigram.record$freq * trigram.record$discount
prob.final <- freq.eff / freq.tot
} else {
# trigram not found -> check bigram
input.uni <- gsub(".* ", "", input)
# get the leftover probability 'beta'
beta <- subset(trigram.beta, bigram == input)$probLeftover
bigram.in <- subset(bigram.wf, unigram == input.uni)
bigram.record <- subset(bigram.in, lastword == predict)
if(nrow(bigram.record) >0 ) {
# bigram found in model
# considered only when lastword not found in trigram
bigram.in.remain <- bigram.in[!(bigram.in$lastword %in% trigram.in$lastword),]
freq.tot <- sum(bigram.in$freq)
freq.eff <- sum(bigram.in.remain$freq*bigram.in.remain$discount)
alpha <- beta / (freq.eff / freq.tot)
freq.eff.record <- bigram.record$freq * bigram.record$discount
prob.final <- alpha * (freq.eff.record / freq.tot)
} else {
# only hope in unigram !
unigram.in <- unigram.wf
unigram.record <- subset(unigram.wf, words == predict)
if(nrow(unigram.record) > 0 ) {
# found in unigram
unigram.in.remain <- unigram.in[!(unigram.in$words %in% trigram.in$lastword),]
freq.tot <- sum(unigram.in$freq)
freq.eff <- sum(unigram.in.remain$freq*unigram.in.remain$discount)
alpha <- beta / (freq.eff / freq.tot)
freq.eff.record <- unigram.record$freq * unigram.record$discount
prob.final <- alpha * (freq.eff.record / freq.tot)
} else {
stop(sprintf("predictor [%s] not found in trigram model", predict))
}
}
}
} else {
sprintf("input [%s] not found in trigram model.", input)
input.uni <- gsub(".* ", "", input)
probBigramPredictor(input.uni, predict)
}
# return final probability
prob.final
}
probBigramPredictor <- function(input, predict) {
#
# single word input plus a predict word, output the probability
# katz back-off with good-turing discounting
#
bigram.in <- subset(bigram.wf, unigram == input)
if(nrow(bigram.in) > 0) {
bigram.record <- subset(bigram.in, lastword == predict)
if(nrow(bigram.record) > 0) {
# find bigram record
freq.tot <- sum(bigram.in$freq)
freq.eff <- bigram.record$freq * bigram.record$discount
prob.final <- freq.eff / freq.tot
} else {
# not find bigram record, go to unigram
beta <- subset(bigram.beta, unigram == input)$probLeftover
unigram.in <- unigram.wf
unigram.record <- subset(unigram.wf, words == predict)
if(nrow(unigram.record) > 0) {
# found in unigram
unigram.in.remain <- unigram.in[!(unigram.in$words %in% bigram.in$lastwords),]
freq.tot <- sum(unigram.in$freq)
freq.eff <- sum(unigram.in.remain$freq * unigram.in.remain$discount)
alpha <- beta / (freq.eff / freq.tot)
freq.eff.record <- unigram.record$freq * unigram.record$discount
prob.final <- alpha * (freq.eff.record / freq.tot)
} else {
stop(sprintf("predictor [%s] not found in bigram model", predict))
}
}
} else {
sprintf("input [%s] not found in bigram model.", input)
probUnigramPredictor(predict)
}
prob.final
}
probUnigramPredictor <- function(predict) {
#
# output probability of given predict word with unigram data
#
unigram.in <- unigram.wf
unigram.record <- subset(unigram.wf, words == predict)
if(nrow(unigram.record) > 0) {
freq.tot <- sum(unigram.in)
freq.eff <- unigram.record$freq * unigram.record$discount
prob.final <- freq.eff / freq.tot
} else {
# any predicted word should be found in the training corpus
# otherwise no way to predict
stop(sprintf("predictor [%s] not found in unigram model", predict))
}
prob.final
}
library(dplyr)
trigram.wf <- read.csv("../trial-ngram/trigram-wf.csv", stringsAsFactors = FALSE)
trigram.fft <- read.csv("../trial-ngram/trigram-fft.csv")
trigram.wf$bigram <- gsub(" [^ ]+$", "", trigram.wf$words)
trigram.wf$unigram <- gsub(".* ", "", trigram.wf$bigram)
trigram.wf$lastword <- gsub(".* ", "", trigram.wf$words)
trigram.wf <- merge(trigram.wf, trigram.fft[,c(1,3)], by = "freq")
# trigram leftover probability
trigram.beta <- trigram.wf %>% group_by(bigram) %>%
summarise(probLeftover = probLeftover(freq, discount)) %>%
as.data.frame()
bigram.wf <- read.csv("../trial-ngram/bigram-wf.csv", stringsAsFactors = FALSE)
bigram.fft <- read.csv("../trial-ngram/bigram-fft.csv")
bigram.wf$unigram <- gsub(" .*", "", bigram.wf$words)
bigram.wf$lastword <- gsub(".* ", "", bigram.wf$words)
bigram.wf <- merge(bigram.wf, bigram.fft[,c(1,3)], by = "freq")
# bigram leftover probability
bigram.beta <- bigram.wf %>% group_by(unigram) %>%
summarise(probLeftover = probLeftover(freq, discount)) %>%
as.data.frame()
unigram.wf <- read.csv("../trial-ngram/unigram-wf.csv", stringsAsFactors = FALSE)
unigram.fft <- read.csv("../trial-ngram/unigram-fft.csv")
unigram.wf <- merge(unigram.wf, unigram.fft[,c(1,3)], by = "freq")
# simple version of prediction function
predict.unigram <- arrange(unigram.wf, desc(freq * discount)) %>%
select(words) %>% head(10)
predict.unigram <- predict.unigram$words
nextWordPrediction <- function(rawinput, numRecommend = 6) {
input.list <- cleanInput(rawinput)
input <- input.list[[1]]
isTrigram <- input.list[[2]]
if(isTrigram == TRUE) {
# pre-selection of 20 words as candidate predict word
# bare Good-Turing discounted probabilities are used
trigram.in <- subset(trigram.wf, bigram == input)
predict.trigram <- arrange(trigram.in, desc(freq * discount)) %>%
select(lastword) %>% head(10)
predict.trigram <- predict.trigram$lastword
input.uni <- gsub(".* ", "", input)
bigram.in <- subset(bigram.wf, unigram == input.uni)
predict.bigram <- arrange(bigram.in, desc(freq * discount)) %>%
select(lastword) %>% head(10)
predict.bigram <- predict.bigram$lastword
predict.vector <- unique(c(predict.trigram, predict.bigram, predict.unigram))
# katz back-off for advanced search
prob.vec <- sapply(predict.vector, function(x) probTrigramPredictor(input, x))
prob.df <- data.frame(predictor = predict.vector, probability = prob.vec)
wordRecommend <- arrange(prob.df, desc(prob.vec)) %>% head(numRecommend) %>% t()
} else {
input.uni <- gsub(".* ", "", input)
bigram.in <- subset(bigram.wf, unigram == input.uni)
predict.bigram <- bigram.in$lastword
predict.vector <- unique(c(predict.bigram, predict.unigram))
# katz back-off
prob.vec <- sapply(predict.vector, function(x) probBigramPredictor(input.uni, x))
prob.df <- data.frame(predictor = predict.vector, probability = prob.vec)
wordRecommend <- arrange(prob.df, desc(prob.vec)) %>% head(numRecommend) %>% t()
}
# vector of suggested words with probability
wordRecommend
}
probTrigramPredictor("The guy in front of me just bought a pound of bacon, a bouquet, and a case of
", "beer")
probTrigramPredictor("case of", "beer")
probTrigramPredictor("case of", "soda")
probTrigramPredictor("case of", "pretzels")
probTrigramPredictor("case of", "cheese")
probTrigramPredictor("mean the", "best")
probTrigramPredictor("mean the", "world")
probTrigramPredictor("mean the", "universe")
probTrigramPredictor("mean the", "most")
probTrigramPredictor("me the", "smelliest")
probTrigramPredictor("me the", "bluest")
probTrigramPredictor("me the", "saddest")
probTrigramPredictor("me the", "happiest")
probTrigramPredictor("but the", "referees")
probTrigramPredictor("but the", "players")
probTrigramPredictor("but the", "denfense")
probTrigramPredictor("but the", "defense")
probTrigramPredictor("but the", "crowd")
probTrigramPredictor("at the", "movies")
probTrigramPredictor("at the", "mall")
probTrigramPredictor("at the", "grocery")
probTrigramPredictor("at the", "beach")
probTrigramPredictor("on my", "motorcycle")
probTrigramPredictor("on my", "phone")
probTrigramPredictor("on my", "horse")
probTrigramPredictor("on my", "way")
probTrigramPredictor("quite some", "years")
probTrigramPredictor("quite some", "time")
probTrigramPredictor("quite some", "weeks")
probTrigramPredictor("quite some", "thing")
probTrigramPredictor("I love", "you")
probTrigramPredictor("i love", "you")
probTrigramPredictor("his little", "ears")
probTrigramPredictor("his little", "toes")
probTrigramPredictor("his little", "fingers")
probTrigramPredictor("his little", "eyes")
probTrigramPredictor("during the", "hard")
probTrigramPredictor("during the", "worse")
probTrigramPredictor("during the", "bad")
probTrigramPredictor("during the", "sad")
probTrigramPredictor("must be", "callous")
probTrigramPredictor("must be", "insensitive")
probTrigramPredictor("must be", "asleep")
probTrigramPredictor("must be", "insane")
q()
